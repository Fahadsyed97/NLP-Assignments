{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nnltk.download()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:50:53.793190Z","iopub.execute_input":"2022-08-26T15:50:53.794046Z","iopub.status.idle":"2022-08-26T15:51:25.799466Z","shell.execute_reply.started":"2022-08-26T15:50:53.793954Z","shell.execute_reply":"2022-08-26T15:51:25.798461Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:25.801437Z","iopub.execute_input":"2022-08-26T15:51:25.803920Z","iopub.status.idle":"2022-08-26T15:51:26.059473Z","shell.execute_reply.started":"2022-08-26T15:51:25.803876Z","shell.execute_reply":"2022-08-26T15:51:26.058134Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Loading Data**","metadata":{}},{"cell_type":"code","source":"df= pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', header=None)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:26.063085Z","iopub.execute_input":"2022-08-26T15:51:26.063396Z","iopub.status.idle":"2022-08-26T15:51:36.003488Z","shell.execute_reply.started":"2022-08-26T15:51:26.063367Z","shell.execute_reply":"2022-08-26T15:51:36.002486Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:36.005845Z","iopub.execute_input":"2022-08-26T15:51:36.006148Z","iopub.status.idle":"2022-08-26T15:51:36.031991Z","shell.execute_reply.started":"2022-08-26T15:51:36.006120Z","shell.execute_reply":"2022-08-26T15:51:36.031085Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.columns = [\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:36.035747Z","iopub.execute_input":"2022-08-26T15:51:36.036094Z","iopub.status.idle":"2022-08-26T15:51:37.134023Z","shell.execute_reply.started":"2022-08-26T15:51:36.036065Z","shell.execute_reply":"2022-08-26T15:51:37.131764Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.135772Z","iopub.execute_input":"2022-08-26T15:51:37.136334Z","iopub.status.idle":"2022-08-26T15:51:37.158727Z","shell.execute_reply.started":"2022-08-26T15:51:37.136293Z","shell.execute_reply":"2022-08-26T15:51:37.157649Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df = df[['target','text']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.160947Z","iopub.execute_input":"2022-08-26T15:51:37.161883Z","iopub.status.idle":"2022-08-26T15:51:37.202650Z","shell.execute_reply.started":"2022-08-26T15:51:37.161842Z","shell.execute_reply":"2022-08-26T15:51:37.201663Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df_pos = df[df['target'] == 4]\ndf_neg = df[df['target'] == 0]\nprint(len(df_pos), len(df_neg))","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.204226Z","iopub.execute_input":"2022-08-26T15:51:37.204593Z","iopub.status.idle":"2022-08-26T15:51:37.295348Z","shell.execute_reply.started":"2022-08-26T15:51:37.204556Z","shell.execute_reply":"2022-08-26T15:51:37.294340Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_pos = df_pos.iloc[:int(len(df_pos)/4)]\ndf_neg = df_neg.iloc[:int(len(df_neg)/4)]\nprint(len(df_pos), len(df_neg))","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.296875Z","iopub.execute_input":"2022-08-26T15:51:37.297229Z","iopub.status.idle":"2022-08-26T15:51:37.305464Z","shell.execute_reply.started":"2022-08-26T15:51:37.297192Z","shell.execute_reply":"2022-08-26T15:51:37.304476Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([df_pos, df_neg])\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.310853Z","iopub.execute_input":"2022-08-26T15:51:37.311848Z","iopub.status.idle":"2022-08-26T15:51:37.329305Z","shell.execute_reply.started":"2022-08-26T15:51:37.311774Z","shell.execute_reply":"2022-08-26T15:51:37.328282Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\n**Tokenization**","metadata":{}},{"cell_type":"code","source":"tk = TweetTokenizer(reduce_len=True)\n\ndata = []\n\nX = df['text'].tolist()\nY = df['target'].tolist()\n\nfor x, y in zip(X, Y):\n    if y == 4:\n        data.append((tk.tokenize(x), 1))\n    else:\n        data.append((tk.tokenize(x), 0))\n\ndata[:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:51:37.331644Z","iopub.execute_input":"2022-08-26T15:51:37.332685Z","iopub.status.idle":"2022-08-26T15:52:13.147251Z","shell.execute_reply.started":"2022-08-26T15:51:37.332639Z","shell.execute_reply":"2022-08-26T15:52:13.146138Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatization**","metadata":{}},{"cell_type":"code","source":"def lemmatize_sentence(tokens):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_sentence = []\n    for word, tag in pos_tag(tokens):\n        if tag.startswith('NN'):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n    return lemmatized_sentence\n\nlemmatize_sentence(data[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:52:13.149019Z","iopub.execute_input":"2022-08-26T15:52:13.149388Z","iopub.status.idle":"2022-08-26T15:52:14.840287Z","shell.execute_reply.started":"2022-08-26T15:52:13.149351Z","shell.execute_reply":"2022-08-26T15:52:14.839170Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Stopwords**","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nprint(stop_words)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:52:14.841943Z","iopub.execute_input":"2022-08-26T15:52:14.842327Z","iopub.status.idle":"2022-08-26T15:52:14.850171Z","shell.execute_reply.started":"2022-08-26T15:52:14.842290Z","shell.execute_reply":"2022-08-26T15:52:14.849027Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"**Cleaning the text**","metadata":{}},{"cell_type":"code","source":"import re, string\n\ndef remove_noise(tweet_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n\n    for token, tag in nltk.pos_tag(tweet_tokens):\n        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n    return cleaned_tokens\n\nremove_noise(data[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:52:14.851927Z","iopub.execute_input":"2022-08-26T15:52:14.852504Z","iopub.status.idle":"2022-08-26T15:52:14.867161Z","shell.execute_reply.started":"2022-08-26T15:52:14.852465Z","shell.execute_reply":"2022-08-26T15:52:14.865871Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def list_to_dict(cleaned_tokens):\n    return dict([token, True] for token in cleaned_tokens)\n\ncleaned_tokens_list = []\n\nfor tokens, label in data:\n    cleaned_tokens_list.append((remove_noise(tokens), label))\n\ncleaned_tokens_list[1]","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:52:14.869058Z","iopub.execute_input":"2022-08-26T15:52:14.869588Z","iopub.status.idle":"2022-08-26T15:58:25.950094Z","shell.execute_reply.started":"2022-08-26T15:52:14.869545Z","shell.execute_reply":"2022-08-26T15:58:25.948979Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"newdf = pd.DataFrame(cleaned_tokens_list)\nnewdf['target'] = Y\nnewdf = newdf[[0,'target']]\nnewdf.columns=['text', 'target']\nnewdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:58:25.951717Z","iopub.execute_input":"2022-08-26T15:58:25.952110Z","iopub.status.idle":"2022-08-26T15:58:26.231249Z","shell.execute_reply.started":"2022-08-26T15:58:25.952074Z","shell.execute_reply":"2022-08-26T15:58:26.230152Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"newdf['text'] = newdf['text'].apply(lambda x: ' '.join([w for w in x]))","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:58:26.232845Z","iopub.execute_input":"2022-08-26T15:58:26.233214Z","iopub.status.idle":"2022-08-26T15:58:26.832985Z","shell.execute_reply.started":"2022-08-26T15:58:26.233178Z","shell.execute_reply":"2022-08-26T15:58:26.832005Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"newdf['target'] = newdf['target'].replace(4,1)\nnewdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:58:26.834298Z","iopub.execute_input":"2022-08-26T15:58:26.834666Z","iopub.status.idle":"2022-08-26T15:58:26.850022Z","shell.execute_reply.started":"2022-08-26T15:58:26.834624Z","shell.execute_reply":"2022-08-26T15:58:26.849158Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\n\npositive_words = []\nnegative_words = []\n\nfor i in range(len(cleaned_tokens_list)):\n    if cleaned_tokens_list[i][1] == 1:\n        positive_words.extend(cleaned_tokens_list[i][0])\n    else:\n        negative_words.extend(cleaned_tokens_list[i][0])\n\ndef wordcloud_draw(data, color = 'black'):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                          background_color = color,\n                          width = 2500,\n                          height = 2000\n                         ).generate(' '.join(data))\n    plt.figure(1, figsize = (13, 13))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nprint(\"Positive words\")\nwordcloud_draw(positive_words, 'white')\nprint(\"Negative words\")\nwordcloud_draw(negative_words)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:58:26.851278Z","iopub.execute_input":"2022-08-26T15:58:26.851717Z","iopub.status.idle":"2022-08-26T15:59:07.828483Z","shell.execute_reply.started":"2022-08-26T15:58:26.851679Z","shell.execute_reply":"2022-08-26T15:59:07.827645Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Train Test Split**","metadata":{}},{"cell_type":"code","source":"train, test = train_test_split(newdf, test_size=0.2, random_state=1)\nX_train = train['text'].values\nX_test = test['text'].values\ny_train = train['target']\ny_test = test['target']","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:07.829899Z","iopub.execute_input":"2022-08-26T15:59:07.830557Z","iopub.status.idle":"2022-08-26T15:59:07.890768Z","shell.execute_reply.started":"2022-08-26T15:59:07.830520Z","shell.execute_reply":"2022-08-26T15:59:07.889817Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**TF-IDF Vectorizer**","metadata":{}},{"cell_type":"code","source":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\nvectoriser.fit(X_train)\nprint(f'Vectoriser fitted.')\nprint('No. of feature_words: ', len(vectoriser.get_feature_names()))","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:07.892247Z","iopub.execute_input":"2022-08-26T15:59:07.893436Z","iopub.status.idle":"2022-08-26T15:59:19.894067Z","shell.execute_reply.started":"2022-08-26T15:59:07.893394Z","shell.execute_reply":"2022-08-26T15:59:19.892989Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train = vectoriser.transform(X_train)\nX_test  = vectoriser.transform(X_test)\nprint(f'Data Transformed.')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:19.895705Z","iopub.execute_input":"2022-08-26T15:59:19.896118Z","iopub.status.idle":"2022-08-26T15:59:30.421851Z","shell.execute_reply.started":"2022-08-26T15:59:19.896079Z","shell.execute_reply":"2022-08-26T15:59:30.420014Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Model Evaluation","metadata":{}},{"cell_type":"code","source":"def model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    # Compute and plot the Confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    categories  = ['Negative','Positive']\n    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:30.423144Z","iopub.execute_input":"2022-08-26T15:59:30.423521Z","iopub.status.idle":"2022-08-26T15:59:30.432983Z","shell.execute_reply.started":"2022-08-26T15:59:30.423484Z","shell.execute_reply":"2022-08-26T15:59:30.432017Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**LinearSVC**","metadata":{}},{"cell_type":"code","source":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_Evaluate(SVCmodel)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:30.434474Z","iopub.execute_input":"2022-08-26T15:59:30.435109Z","iopub.status.idle":"2022-08-26T15:59:36.558093Z","shell.execute_reply.started":"2022-08-26T15:59:30.435049Z","shell.execute_reply":"2022-08-26T15:59:36.557224Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression**","metadata":{}},{"cell_type":"code","source":"LRmodel = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nLRmodel.fit(X_train, y_train)\nmodel_Evaluate(LRmodel)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T15:59:36.559763Z","iopub.execute_input":"2022-08-26T15:59:36.560094Z","iopub.status.idle":"2022-08-26T16:00:31.348227Z","shell.execute_reply.started":"2022-08-26T15:59:36.560066Z","shell.execute_reply":"2022-08-26T16:00:31.339156Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**XGBoost**","metadata":{}},{"cell_type":"code","source":"XGB = XGBClassifier()\nXGB.fit(X_train,y_train)\nmodel_Evaluate(XGB)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T16:00:31.353218Z","iopub.execute_input":"2022-08-26T16:00:31.356236Z","iopub.status.idle":"2022-08-26T16:04:41.794851Z","shell.execute_reply.started":"2022-08-26T16:00:31.356184Z","shell.execute_reply":"2022-08-26T16:04:41.793857Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Bernoulli Naive Bayes**","metadata":{}},{"cell_type":"code","source":"BNBmodel = BernoulliNB(alpha = 2)\nBNBmodel.fit(X_train, y_train)\nmodel_Evaluate(BNBmodel)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T16:04:41.796627Z","iopub.execute_input":"2022-08-26T16:04:41.797033Z","iopub.status.idle":"2022-08-26T16:04:42.411759Z","shell.execute_reply.started":"2022-08-26T16:04:41.796993Z","shell.execute_reply":"2022-08-26T16:04:42.410749Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"**Looking at the accuracy and the confusion matrix we can say that Logistic Regression is performing better compared to the other models shown above.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}